{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45b1de2c",
   "metadata": {},
   "source": [
    "# 단어 토큰화(Word Tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3776dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['아주대학교', '일반', '대학원', '수학과', '데이터', '사이언스', '전공에', '소속된', '이규철입니다.']\n"
     ]
    }
   ],
   "source": [
    "sentence = '아주대학교 일반 대학원 수학과 데이터 사이언스 전공에 소속된 이규철입니다.'\n",
    "word_tokenization = sentence.split()\n",
    "print(word_tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b38e1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'started', 'studying', 'natural', 'language', 'processing.']\n"
     ]
    }
   ],
   "source": [
    "sentence = 'I started studying natural language processing.'\n",
    "word_tokenization = sentence.split()\n",
    "print(word_tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c749da4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', 'are', 'students!']\n",
      "[\"We're\", 'students!!']\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "sentence1 = \"We are students!\"\n",
    "sentence2 = \"We're students!!\"\n",
    "word_tokenization1 = sentence1.split()\n",
    "word_tokenization2 = sentence2.split()\n",
    "print(word_tokenization1)\n",
    "print(word_tokenization2)\n",
    "print(word_tokenization1[2] == word_tokenization2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1e13363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'price', 'of', 'Nintendo', 'is', '29999']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "sentence = \"The price of Nintendo is $299.99.\"\n",
    "new_sentence = re.sub(r'[^\\w\\s]', '', sentence)\n",
    "word_tokenization = new_sentence.split()\n",
    "print(word_tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b8e554e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'m\", 'so', 'happy', 'to', 'be', 'with', 'my', 'hamster', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "sentence = \"I'm so happy to be with my hamster.\"\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4058f2",
   "metadata": {},
   "source": [
    "# 문장 토큰화(Sentence Tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91dc1d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['아이피 192', '168', '56', '31 서버에 들어가서 로그 파일을 저장해줘', ' 내 이메일로 결과 좀 보내줘', ' 밥 먹으러 가자', '']\n",
      "['햄스터는 너무 귀여워', ' 너가 더 귀여워', ' 고마워', '']\n",
      "['너 포켓몬스터 게임을 하니', ' 아니 넌 하니', ' 너 요즘 어떻게 지내', '']\n"
     ]
    }
   ],
   "source": [
    "text1 = \"아이피 192.168.56.31 서버에 들어가서 로그 파일을 저장해줘. 내 이메일로 결과 좀 보내줘. 밥 먹으러 가자.\"\n",
    "text2 = \"햄스터는 너무 귀여워! 너가 더 귀여워! 고마워!\"\n",
    "text3 = \"너 포켓몬스터 게임을 하니? 아니 넌 하니? 너 요즘 어떻게 지내?\"\n",
    "sentence_tokenization1 = text1.split('.')\n",
    "sentence_tokenization2 = text2.split('!')\n",
    "sentence_tokenization3 = text3.split('?')\n",
    "print(sentence_tokenization1)\n",
    "print(sentence_tokenization2)\n",
    "print(sentence_tokenization3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8123f767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['아이피 192.168.56.31 서버에 들어가서 로그 파일을 저장해줘.', '내 이메일로 결과 좀 보내줘.', '밥 먹으러 가자.']\n",
      "['햄스터는 너무 귀여워!', '너가 더 귀여워!', '고마워!']\n",
      "['너 포켓몬스터 게임을 하니?', '아니 넌 하니?', '너 요즘 어떻게 지내?']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text1 = \"아이피 192.168.56.31 서버에 들어가서 로그 파일을 저장해줘. 내 이메일로 결과 좀 보내줘. 밥 먹으러 가자.\"\n",
    "text2 = \"햄스터는 너무 귀여워! 너가 더 귀여워! 고마워!\"\n",
    "text3 = \"너 포켓몬스터 게임을 하니? 아니 넌 하니? 너 요즘 어떻게 지내?\"\n",
    "sentence_tokenization1 = sent_tokenize(text1)\n",
    "sentence_tokenization2 = sent_tokenize(text2)\n",
    "sentence_tokenization3 = sent_tokenize(text3)\n",
    "print(sentence_tokenization1)\n",
    "print(sentence_tokenization2)\n",
    "print(sentence_tokenization3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a678c62d",
   "metadata": {},
   "source": [
    "# 영어에서의 토큰화(Tokenization in English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4650db2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print('Hello' == 'hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ada0922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, world!\n",
      "['hello', ',', 'world', '!']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "sentence = \"Hello, World!\"\n",
    "lower_sentence = sentence.lower()\n",
    "print(lower_sentence)\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(lower_sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90e1419",
   "metadata": {},
   "source": [
    "# 한국어에서의 토큰화(Tokenization in Korean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b20c58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iamagraduatestudent.\n",
      "나는대학원생이다.\n"
     ]
    }
   ],
   "source": [
    "en_sentence = \"I am a graduate student.\"\n",
    "new_en_sentence = en_sentence.replace(\" \", \"\")\n",
    "ko_sentence = \"나는 대학원생이다.\"\n",
    "new_ko_sentence = ko_sentence.replace(\" \", \"\")\n",
    "print(new_en_sentence)\n",
    "print(new_ko_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2727820c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['나는', '햄스터가', '좋아.', '햄스터는', '해바라기씨를', '좋아해.', '햄스터를', '다시', '키우고싶어.']\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "text = \"나는 햄스터가 좋아. 햄스터는 해바라기씨를 좋아해. 햄스터를 다시 키우고싶어.\"\n",
    "word_tokenization = text.split()\n",
    "print(word_tokenization)\n",
    "print(word_tokenization[1] == word_tokenization[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69beeb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "명사 추출: ['나', '햄스터', '햄스터', '해바라기씨', '햄스터']\n",
      "형태소 추출: ['나', '는', '햄스터', '가', '좋', '아', '.', '햄스터', '는', '해바라기씨', '를', '좋', '아', '하', '어', '.', '햄스터', '를', '다시', '키우', '고', '싶', '어', '.']\n",
      "품사 부착: [('나', 'N'), ('는', 'J'), ('햄스터', 'N'), ('가', 'J'), ('좋', 'P'), ('아', 'E'), ('.', 'S'), ('햄스터', 'N'), ('는', 'J'), ('해바라기씨', 'N'), ('를', 'J'), ('좋', 'P'), ('아', 'E'), ('하', 'P'), ('어', 'E'), ('.', 'S'), ('햄스터', 'N'), ('를', 'J'), ('다시', 'M'), ('키우', 'P'), ('고', 'E'), ('싶', 'P'), ('어', 'E'), ('.', 'S')]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Hannanum\n",
    "\n",
    "hannanum = Hannanum()\n",
    "text = \"나는 햄스터가 좋아. 햄스터는 해바라기씨를 좋아해. 햄스터를 다시 키우고싶어.\"\n",
    "nouns_tokens = hannanum.nouns(text)\n",
    "morphs_tokens = hannanum.morphs(text)\n",
    "pos_tokens = hannanum.pos(text)\n",
    "print('명사 추출:', nouns_tokens)\n",
    "print('형태소 추출:', morphs_tokens)\n",
    "print('품사 부착:', pos_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73daaf14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "명사 추출: ['나', '햄스터', '해바라기', '해바라기씨', '씨']\n",
      "형태소 추출: ['나', '는', '햄스터', '가', '좋', '아', '.', '햄스터', '는', '해바라기', '씨', '를', '좋아하', '어', '.', '햄스터', '를', '다시', '키우', '고', '싶', '어', '.']\n",
      "품사 부착: [('나', 'NP'), ('는', 'JX'), ('햄스터', 'NNG'), ('가', 'JKS'), ('좋', 'VA'), ('아', 'ECD'), ('.', 'SF'), ('햄스터', 'NNG'), ('는', 'JX'), ('해바라기', 'NNG'), ('씨', 'NNB'), ('를', 'JKO'), ('좋아하', 'VV'), ('어', 'ECS'), ('.', 'SF'), ('햄스터', 'NNG'), ('를', 'JKO'), ('다시', 'MAG'), ('키우', 'VV'), ('고', 'ECE'), ('싶', 'VXA'), ('어', 'ECD'), ('.', 'SF')]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Kkma\n",
    "\n",
    "kkma = Kkma()\n",
    "text = \"나는 햄스터가 좋아. 햄스터는 해바라기씨를 좋아해. 햄스터를 다시 키우고싶어.\"\n",
    "nouns_tokens = kkma.nouns(text)\n",
    "morphs_tokens = kkma.morphs(text)\n",
    "pos_tokens = kkma.pos(text)\n",
    "print('명사 추출:', nouns_tokens)\n",
    "print('형태소 추출:', morphs_tokens)\n",
    "print('품사 부착:', pos_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7aa1d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "명사 추출: ['햄스터', '햄스터', '해바라기', '씨', '햄스터']\n",
      "형태소 추출: ['나', '는', '햄스터', '가', '좋', '아', '.', '햄스터', '는', '해바라기', '씨', '를', '좋아하', '아', '.', '햄스터', '를', '다시', '키우', '고', '싶', '어', '.']\n",
      "품사 부착: [('나', 'NP'), ('는', 'JX'), ('햄스터', 'NNP'), ('가', 'JKS'), ('좋', 'VA'), ('아', 'EF'), ('.', 'SF'), ('햄스터', 'NNP'), ('는', 'JX'), ('해바라기', 'NNP'), ('씨', 'NNB'), ('를', 'JKO'), ('좋아하', 'VV'), ('아', 'EF'), ('.', 'SF'), ('햄스터', 'NNP'), ('를', 'JKO'), ('다시', 'MAG'), ('키우', 'VV'), ('고', 'EC'), ('싶', 'VX'), ('어', 'EF'), ('.', 'SF')]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Komoran\n",
    "\n",
    "komoran = Komoran()\n",
    "text = \"나는 햄스터가 좋아. 햄스터는 해바라기씨를 좋아해. 햄스터를 다시 키우고싶어.\"\n",
    "nouns_tokens = komoran.nouns(text)\n",
    "morphs_tokens = komoran.morphs(text)\n",
    "pos_tokens = komoran.pos(text)\n",
    "print('명사 추출:', nouns_tokens)\n",
    "print('형태소 추출:', morphs_tokens)\n",
    "print('품사 부착:', pos_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc1e37b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "명사 추출: ['나', '햄스터', '햄스터', '해바라기씨', '햄스터', '다시']\n",
      "형태소 추출: ['나', '는', '햄스터', '가', '좋아', '.', '햄스터', '는', '해바라기씨', '를', '좋아해', '.', '햄스터', '를', '다시', '키우고싶어', '.']\n",
      "품사 부착: [('나', 'Noun'), ('는', 'Josa'), ('햄스터', 'Noun'), ('가', 'Josa'), ('좋아', 'Adjective'), ('.', 'Punctuation'), ('햄스터', 'Noun'), ('는', 'Josa'), ('해바라기씨', 'Noun'), ('를', 'Josa'), ('좋아해', 'Adjective'), ('.', 'Punctuation'), ('햄스터', 'Noun'), ('를', 'Josa'), ('다시', 'Noun'), ('키우고싶어', 'Verb'), ('.', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "text = \"나는 햄스터가 좋아. 햄스터는 해바라기씨를 좋아해. 햄스터를 다시 키우고싶어.\"\n",
    "nouns_tokens = okt.nouns(text)\n",
    "morphs_tokens = okt.morphs(text)\n",
    "pos_tokens = okt.pos(text)\n",
    "print('명사 추출:', nouns_tokens)\n",
    "print('형태소 추출:', morphs_tokens)\n",
    "print('품사 부착:', pos_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a9a75f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
